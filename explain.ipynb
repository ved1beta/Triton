{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPOdV7yeW4V+i3+uWoah/8c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ved1beta/Triton/blob/main/explain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cVY_yt77sQoS"
      },
      "outputs": [],
      "source": [
        "# First, create a new notebook in Google Colab and paste this code into different cells\n",
        "\n",
        "# Cell 1 - Install dependencies\n",
        "%%capture\n",
        "!pip install triton\n",
        "\n",
        "# Cell 2 - Import libraries and check GPU\n",
        "import torch\n",
        "import triton\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Cell 3 - Neural Network Implementation\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "@triton.jit\n",
        "def triton_nn_kernel(\n",
        "    input_ptr,\n",
        "    weight_ptr,\n",
        "    output_ptr,\n",
        "    batch_size,\n",
        "    in_features,\n",
        "    out_features,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "    pid = tl.program_id(axis=0)\n",
        "\n",
        "    batch_idx = pid // out_features\n",
        "    out_idx = pid % out_features\n",
        "\n",
        "    offsets = tl.arange(0, BLOCK_SIZE)\n",
        "\n",
        "    acc = 0.0\n",
        "\n",
        "    input_block_ptr = input_ptr + batch_idx * in_features\n",
        "    weight_block_ptr = weight_ptr + out_idx * in_features\n",
        "\n",
        "    for block_start in range(0, in_features, BLOCK_SIZE):\n",
        "        block_mask = block_start + offsets < in_features\n",
        "\n",
        "        input_block = tl.load(input_block_ptr + block_start + offsets, mask=block_mask, other=0.0)\n",
        "        weight_block = tl.load(weight_block_ptr + block_start + offsets, mask=block_mask, other=0.0)\n",
        "\n",
        "        acc += tl.sum(input_block * weight_block * block_mask, axis=0)\n",
        "\n",
        "    output_offset = batch_idx * out_features + out_idx\n",
        "    tl.store(output_ptr + output_offset, acc)\n",
        "\n",
        "class TritonNeuralNetwork(torch.nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, out_features):\n",
        "        super().__init__()\n",
        "        self.layer1 = torch.nn.Linear(in_features, hidden_features)\n",
        "        self.layer2 = torch.nn.Linear(hidden_features, out_features)\n",
        "        self.block_size = 32\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) > 2:\n",
        "            x = x.view(x.size(0), -1)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        output = torch.empty((batch_size, self.layer1.out_features), device=x.device)\n",
        "\n",
        "        grid = (batch_size * self.layer1.out_features,)\n",
        "        triton_nn_kernel[grid](\n",
        "            x.contiguous(),\n",
        "            self.layer1.weight.contiguous(),\n",
        "            output,\n",
        "            batch_size,\n",
        "            self.layer1.in_features,\n",
        "            self.layer1.out_features,\n",
        "            self.block_size,\n",
        "        )\n",
        "\n",
        "        hidden = F.relu(output)\n",
        "        return self.layer2(hidden)\n",
        "\n",
        "# Cell 4 - Training Setup and Data Loading\n",
        "# Set random seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST('/content/data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('/content/data', train=False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize model\n",
        "model = TritonNeuralNetwork(\n",
        "    in_features=28*28,\n",
        "    hidden_features=256,\n",
        "    out_features=10\n",
        ").cuda()\n",
        "\n",
        "# Cell 5 - Training Function\n",
        "def train_and_evaluate(model, train_loader, test_loader, epochs=10, learning_rate=0.01):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    metrics = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'test_loss': [],\n",
        "        'test_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        train_acc = 100. * correct / total\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # Testing\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in test_loader:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                test_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        test_acc = 100. * correct / total\n",
        "        test_loss = test_loss / len(test_loader)\n",
        "\n",
        "        metrics['train_loss'].append(train_loss)\n",
        "        metrics['train_acc'].append(train_acc)\n",
        "        metrics['test_loss'].append(test_loss)\n",
        "        metrics['test_acc'].append(test_acc)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}:')\n",
        "        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n",
        "        print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Cell 6 - Run Training\n",
        "metrics = train_and_evaluate(\n",
        "    model,\n",
        "    train_loader,\n",
        "    test_loader,\n",
        "    epochs=10,\n",
        "    learning_rate=0.001\n",
        ")\n",
        "\n",
        "# Cell 7 - Plot Results\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(metrics['train_loss'], label='Train Loss')\n",
        "plt.plot(metrics['test_loss'], label='Test Loss')\n",
        "plt.title('Loss over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(metrics['train_acc'], label='Train Accuracy')\n",
        "plt.plot(metrics['test_acc'], label='Test Accuracy')\n",
        "plt.title('Accuracy over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zoTtBbqIsYdQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}